{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB dataset\n",
    "\n",
    "This notebook walks through how perform sentament analysis on the IMDB dataset.\n",
    "In this setting, one party has the reviews and the other party has the labels.\n",
    "The party with the labels is helping the party with the reviews train a model\n",
    "without sharing the labels themselves.\n",
    "\n",
    "Before starting, install tf-shell and the dataset.\n",
    "\n",
    "```bash\n",
    "pip install tf-shell\n",
    "pip install tensorflow_hub tensorflow_datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:43:44.632717: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 15:43:44.655330: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tf_shell\n",
    "import tf_shell_ml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# full_range = 33\n",
    "# num_threads = 3\n",
    "\n",
    "# a = list(range(0, 10))\n",
    "# b = list(range(10, 20))\n",
    "# c = list(range(20, 33))\n",
    "\n",
    "# # import math\n",
    "# # r = math.ceil(33 / 13)\n",
    "# # print(r)\n",
    "\n",
    "# aa = [i * 11 for i in a]\n",
    "# bb = [i * 11 for i in b]\n",
    "# cc = [i * 11 for i in c]\n",
    "\n",
    "# aa = [aaa%33 for aaa in aa]\n",
    "# bb = [bbb%33 for bbb in bb]\n",
    "# cc = [ccc%33 for ccc in cc]\n",
    "\n",
    "# print(aa)\n",
    "# print(bb)\n",
    "# print(cc)\n",
    "\n",
    "# print(sorted(aa + bb + cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters for the SHELL encryption library.\n",
    "context = tf_shell.create_context64(\n",
    "    log_n=12,\n",
    "    main_moduli=[288230376151760897, 288230376152137729],\n",
    "    plaintext_modulus=4294991873,\n",
    "    scaling_factor=3,\n",
    "    mul_depth_supported=3,\n",
    "    seed=\"test_seed\",\n",
    ")\n",
    "\n",
    "# Create the secret key for encryption and a rotation key (rotation key is\n",
    "# an auxilary key required for operations like roll or matmul).\n",
    "secret_key = tf_shell.create_key64(context)\n",
    "public_rotation_key = tf_shell.create_rotation_key64(context, secret_key)\n",
    "\n",
    "# The batch size is determined by the ciphertext parameters, specifically the\n",
    "# schemes polynomial's ring degree because tf-shell uses batch axis packing.\n",
    "# Furthermore, two micro-batches to run in parallel.\n",
    "batch_size = context.num_slots\n",
    "\n",
    "use_encryption = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:43:59.362596: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-08-21 15:43:59.363258: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "Label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:44:03.915397: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words: ['', '[UNK]', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'it']\n",
      "Dictionary size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:45:02.520708: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 () count: 193376173\n",
      "Word 1 ([UNK]) count: 1909588\n",
      "Word 2 (the) count: 1640160\n",
      "Word 3 (a) count: 793567\n",
      "Word 4 (and) count: 792777\n",
      "Word 5 (of) count: 713473\n",
      "Word 6 (to) count: 663458\n",
      "Word 7 (is) count: 521799\n",
      "Word 8 (in) count: 454077\n",
      "Word 9 (it) count: 378937\n",
      "Word 10 (i) count: 376537\n",
      "Word 11 (this) count: 372522\n",
      "Word 12 (that) count: 342774\n",
      "Word 13 (br) count: 281142\n",
      "Word 14 (was) count: 238349\n",
      "Word 15 (as) count: 226955\n",
      "Word 16 (for) count: 214889\n",
      "Word 17 (with) count: 215272\n",
      "Word 18 (movie) count: 205534\n",
      "Word 19 (but) count: 204017\n",
      "Word 20 (film) count: 184764\n",
      "Word 21 (on) count: 165747\n",
      "Word 22 (not) count: 147455\n",
      "Word 23 (you) count: 146662\n",
      "Word 24 (are) count: 144779\n",
      "Word 25 (his) count: 143958\n",
      "Word 26 (have) count: 137439\n",
      "Word 27 (be) count: 130597\n",
      "Word 28 (he) count: 129849\n",
      "Word 29 (one) count: 125283\n",
      "Word 30 (its) count: 122528\n",
      "Word 31 (all) count: 115207\n",
      "Word 32 (at) count: 114532\n",
      "Word 33 (by) count: 109142\n",
      "Word 34 (they) count: 104436\n",
      "Word 35 (an) count: 104161\n",
      "Word 36 (who) count: 99445\n",
      "Word 37 (from) count: 98893\n",
      "Word 38 (so) count: 97566\n",
      "Word 39 (like) count: 97295\n",
      "Word 40 (her) count: 87109\n",
      "Word 41 (or) count: 86896\n",
      "Word 42 (just) count: 86562\n",
      "Word 43 (about) count: 84438\n",
      "Word 44 (if) count: 82925\n",
      "Word 45 (out) count: 80828\n",
      "Word 46 (has) count: 80646\n",
      "Word 47 (some) count: 77991\n",
      "Word 48 (there) count: 75393\n",
      "Word 49 (what) count: 75525\n",
      "Word 50 (good) count: 71628\n",
      "Word 51 (when) count: 68912\n",
      "Word 52 (very) count: 68812\n",
      "Word 53 (more) count: 68827\n",
      "Word 54 (my) count: 61716\n",
      "Word 55 (even) count: 61107\n",
      "Word 56 (she) count: 60711\n",
      "Word 57 (would) count: 60265\n",
      "Word 58 (no) count: 59224\n",
      "Word 59 (up) count: 58924\n",
      "Word 60 (time) count: 58084\n",
      "Word 61 (really) count: 57577\n",
      "Word 62 (which) count: 57639\n",
      "Word 63 (only) count: 57295\n",
      "Word 64 (had) count: 55794\n",
      "Word 65 (see) count: 55664\n",
      "Word 66 (were) count: 55603\n",
      "Word 67 (their) count: 55435\n",
      "Word 68 (story) count: 54721\n",
      "Word 69 (can) count: 53968\n",
      "Word 70 (me) count: 52010\n",
      "Word 71 (we) count: 48428\n",
      "Word 72 (than) count: 48394\n",
      "Word 73 (much) count: 46161\n",
      "Word 74 (well) count: 45621\n",
      "Word 75 (been) count: 45647\n",
      "Word 76 (get) count: 45271\n",
      "Word 77 (will) count: 44955\n",
      "Word 78 (do) count: 45128\n",
      "Word 79 (bad) count: 44506\n",
      "Word 80 (also) count: 44674\n",
      "Word 81 (people) count: 44627\n",
      "Word 82 (because) count: 44304\n",
      "Word 83 (into) count: 44234\n",
      "Word 84 (other) count: 44086\n",
      "Word 85 (great) count: 43557\n",
      "Word 86 (first) count: 43347\n",
      "Word 87 (how) count: 43349\n",
      "Word 88 (dont) count: 41896\n",
      "Word 89 (most) count: 41654\n",
      "Word 90 (him) count: 41124\n",
      "Word 91 (then) count: 39063\n",
      "Word 92 (movies) count: 38852\n",
      "Word 93 (make) count: 38622\n",
      "Word 94 (made) count: 38285\n",
      "Word 95 (them) count: 38341\n",
      "Word 96 (films) count: 38079\n",
      "Word 97 (way) count: 37708\n",
      "Word 98 (any) count: 37630\n",
      "Word 99 (could) count: 37582\n",
      "Word 100 (too) count: 37403\n",
      "Word 101 (after) count: 36156\n",
      "Word 102 (characters) count: 35684\n",
      "Word 103 (think) count: 35308\n",
      "Word 104 (watch) count: 34077\n",
      "Word 105 (two) count: 32735\n",
      "Word 106 (many) count: 32700\n",
      "Word 107 (being) count: 32207\n",
      "Word 108 (seen) count: 32030\n",
      "Word 109 (character) count: 31993\n",
      "Word 110 (never) count: 31848\n",
      "Word 111 (plot) count: 31197\n",
      "Word 112 (acting) count: 30859\n",
      "Word 113 (best) count: 30910\n",
      "Word 114 (did) count: 30577\n",
      "Word 115 (love) count: 30510\n",
      "Word 116 (little) count: 30408\n",
      "Word 117 (where) count: 30227\n",
      "Word 118 (life) count: 29449\n",
      "Word 119 (show) count: 29159\n",
      "Word 120 (know) count: 28792\n",
      "Word 121 (ever) count: 28489\n",
      "Word 122 (does) count: 28419\n",
      "Word 123 (your) count: 28378\n",
      "Word 124 (still) count: 27281\n",
      "Word 125 (better) count: 27206\n",
      "Word 126 (over) count: 27275\n",
      "Word 127 (these) count: 26665\n",
      "Word 128 (say) count: 26350\n",
      "Word 129 (while) count: 26310\n",
      "Word 130 (off) count: 25975\n",
      "Word 131 (end) count: 25797\n",
      "Word 132 (man) count: 25800\n",
      "Word 133 (scene) count: 25255\n",
      "Word 134 (here) count: 25047\n",
      "Word 135 (such) count: 25008\n",
      "Word 136 (scenes) count: 24828\n",
      "Word 137 (why) count: 24688\n",
      "Word 138 (go) count: 24822\n",
      "Word 139 (through) count: 24289\n",
      "Word 140 (should) count: 24287\n",
      "Word 141 (something) count: 23949\n",
      "Word 142 (im) count: 23517\n",
      "Word 143 (back) count: 23445\n",
      "Word 144 (those) count: 22815\n",
      "Word 145 (doesnt) count: 22692\n",
      "Word 146 (real) count: 22575\n",
      "Word 147 (thing) count: 22339\n",
      "Word 148 (watching) count: 22373\n",
      "Word 149 (years) count: 22009\n",
      "Word 150 (now) count: 21586\n",
      "Word 151 (didnt) count: 21494\n",
      "Word 152 (though) count: 21345\n",
      "Word 153 (actors) count: 20895\n",
      "Word 154 (makes) count: 20531\n",
      "Word 155 (actually) count: 20524\n",
      "Word 156 (nothing) count: 20552\n",
      "Word 157 (find) count: 20407\n",
      "Word 158 (old) count: 20252\n",
      "Word 159 (another) count: 20236\n",
      "Word 160 (work) count: 20462\n",
      "Word 161 (before) count: 20327\n",
      "Word 162 (new) count: 20287\n",
      "Word 163 (going) count: 20170\n",
      "Word 164 (funny) count: 20065\n",
      "Word 165 (every) count: 20039\n",
      "Word 166 (same) count: 19897\n",
      "Word 167 (look) count: 19660\n",
      "Word 168 (few) count: 19545\n",
      "Word 169 (us) count: 19534\n",
      "Word 170 (lot) count: 19177\n",
      "Word 171 (part) count: 19037\n",
      "Word 172 (director) count: 18893\n",
      "Word 173 (cant) count: 18858\n",
      "Word 174 (again) count: 18753\n",
      "Word 175 (quite) count: 18529\n",
      "Word 176 (cast) count: 18372\n",
      "Word 177 (thats) count: 18225\n",
      "Word 178 (pretty) count: 17873\n",
      "Word 179 (want) count: 17943\n",
      "Word 180 (seems) count: 17566\n",
      "Word 181 (got) count: 17422\n",
      "Word 182 (things) count: 17433\n",
      "Word 183 (young) count: 17194\n",
      "Word 184 (around) count: 17198\n",
      "Word 185 (fact) count: 17099\n",
      "Word 186 (down) count: 16886\n",
      "Word 187 (enough) count: 16926\n",
      "Word 188 (take) count: 16740\n",
      "Word 189 (however) count: 16897\n",
      "Word 190 (may) count: 16737\n",
      "Word 191 (thought) count: 16733\n",
      "Word 192 (between) count: 16411\n",
      "Word 193 (both) count: 16360\n",
      "Word 194 (world) count: 16494\n",
      "Word 195 (own) count: 16244\n",
      "Word 196 (series) count: 16138\n",
      "Word 197 (give) count: 16189\n",
      "Word 198 (original) count: 16250\n",
      "Word 199 (ive) count: 15956\n"
     ]
    }
   ],
   "source": [
    "# Split the training set into 60% and 40% to end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_data, val_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)\n",
    "\n",
    "# Print the first example.\n",
    "for review, label in train_data.take(1):\n",
    "    print(\"Review:\", review.numpy().decode('utf-8'))\n",
    "    print(\"Label:\", label.numpy())\n",
    "\n",
    "epochs = 10\n",
    "train_data = train_data.shuffle(buffer_size=2048).batch(batch_size, drop_remainder=True).repeat(count=epochs)\n",
    "val_data = val_data.shuffle(buffer_size=2048).batch(batch_size, drop_remainder=True)\n",
    "test_data = test_data.shuffle(buffer_size=2048).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "vocab_size = 10000  # This dataset has 92061 unique words.\n",
    "max_length = 250\n",
    "embedding_dim = 16\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(train_data.map(lambda text, label: text))\n",
    "\n",
    "print(\"Most used words:\", vectorize_layer.get_vocabulary()[:10])\n",
    "print(\"Dictionary size:\", len(vectorize_layer.get_vocabulary()))\n",
    "\n",
    "# Count the top n words in the training set.\n",
    "top_n = 200\n",
    "word_counts = np.zeros(top_n, dtype=np.int64)\n",
    "for review, label in train_data:\n",
    "    vectorized_reviews = vectorize_layer(review)\n",
    "    for i in range(len(word_counts)):\n",
    "        counts = tf.where(vectorized_reviews == i, 1, 0)\n",
    "        word_counts[i] += tf.reduce_sum(tf.cast(counts, dtype=tf.int64))\n",
    "\n",
    "for i in range(len(word_counts)):\n",
    "    print(f\"Word {i} ({vectorize_layer.get_vocabulary()[i]}) count: {word_counts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainable layers.\n",
    "embedding_layer = tf_shell_ml.ShellEmbedding(\n",
    "    vocab_size + 1,  # +1 for OOV token.\n",
    "    embedding_dim,\n",
    "    skip_embeddings_below_index=top_n,\n",
    ")\n",
    "# TODO dropout layer?\n",
    "hidden_layer = tf_shell_ml.GlobalAveragePooling1D()\n",
    "# TODO dropout layer?\n",
    "output_layer = tf_shell_ml.ShellDense(\n",
    "    2,\n",
    "    activation=tf.nn.softmax,\n",
    ")\n",
    "\n",
    "layers = [\n",
    "    embedding_layer,\n",
    "    hidden_layer,\n",
    "    output_layer,\n",
    "]\n",
    "\n",
    "loss_fn = tf_shell_ml.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `train_step` function which will be called for each batch on an\n",
    "encrypted batch of labels, y. The function first does a forward on the plaintext\n",
    "image x to compute a predicted label, then does backpropagation using the\n",
    "encrypted label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train_step(x, enc_y):\n",
    "    # Forward pass always in plaintext\n",
    "    y_pred = x\n",
    "    for i, l in enumerate(layers):\n",
    "        y_pred = l(y_pred)\n",
    "\n",
    "    # Backward pass.\n",
    "    dx = loss_fn.grad(enc_y, y_pred)\n",
    "    dJ_dw = []\n",
    "    dJ_dx = [dx,]\n",
    "    for l in reversed(layers):\n",
    "        if isinstance(l, tf_shell_ml.GlobalAveragePooling1D):\n",
    "            dw, dx = l.backward(dJ_dx[-1])\n",
    "        else:\n",
    "            dw, dx = l.backward(dJ_dx[-1], public_rotation_key)\n",
    "        dJ_dw.extend(dw)\n",
    "        dJ_dx.append(dx)\n",
    "\n",
    "    return reversed(dJ_dw)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_wrapper(x_batch, y_batch):\n",
    "    if use_encryption:\n",
    "        # Encrypt the batch of secret labels y.\n",
    "        enc_y_batch = tf_shell.to_encrypted(y_batch, secret_key, context)\n",
    "    else:\n",
    "        enc_y_batch = y_batch\n",
    "\n",
    "    # Run the training step. The top and bottom halves of the batch are\n",
    "    # treated as two separate mini-batches run in parallel.\n",
    "    enc_grads = train_step(x_batch, enc_y_batch)\n",
    "\n",
    "    filtered_layers = [l for l in layers if len(l.weights) > 0]\n",
    "\n",
    "    if use_encryption:\n",
    "        # Decrypt the weight gradients. In practice, the gradients should be\n",
    "        # noised before decrypting.\n",
    "        packed_grads = [tf_shell.to_tensorflow(g, secret_key) for g in enc_grads]\n",
    "        # Unpack the plaintext gradients using the corresponding layer.\n",
    "        grads = [l.unpack(g) for l, g in zip (filtered_layers, packed_grads)]\n",
    "    else:\n",
    "        grads = enc_grads\n",
    "\n",
    "    weights = []\n",
    "    for l in filtered_layers:\n",
    "        weights+=l.weights\n",
    "\n",
    "    # Apply the gradients to the model.\n",
    "    optimizer.apply_gradients(zip(grads, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training loop. Each inner iteration runs two batches of size\n",
    "$2^{12-1}$ simultaneously.\n",
    "\n",
    "Tensorboard can be used to visualize the training progress. See cell output for\n",
    "command to start tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To start tensorboard, run: tensorboard --logdir ./ --host 0.0.0.0\n",
      "\ttensorboard profiling requires: pip install tensorboard_plugin_profile\n",
      "\tvalidation loss: 0.3467586040496826\taccuracy: 0.49951171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:45:05.723558: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-08-21 15:45:05.756578: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-08-21 15:45:05.756607: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 / 30, Time Stamp: 0.4095344543457031\n",
      "WARNING:tensorflow:Error while stopping profiler: Cannot export profiling results. No profiler is running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 16:49:55.676157: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-08-21 16:49:55.723325: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-08-21 16:49:55.723922: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: /workspaces/tf-shell/examples/tflogs/sentiment-20240821-154505/plugins/profile/2024_08_21_16_49_55/e64b0b6b3843.xplane.pb\n",
      "WARNING:tensorflow:Error while stopping profiler: Cannot export profiling results. No profiler is running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain loss: 0.3465079069137573\taccuracy: 0.5030192136764526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 16:49:59.743196: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvalidation loss: 0.34663450717926025\taccuracy: 0.498291015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 16:50:00.033112: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 / 30, Time Stamp: 3894.7336428165436\n",
      "\ttrain loss: 0.3463079631328583\taccuracy: 0.5083984136581421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 17:59:25.388828: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvalidation loss: 0.3463442325592041\taccuracy: 0.508544921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 17:59:25.689975: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2 / 30, Time Stamp: 8060.355618476868\n",
      "\ttrain loss: 0.34611544013023376\taccuracy: 0.5154866576194763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 19:01:54.026599: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvalidation loss: 0.3462379276752472\taccuracy: 0.512451171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 19:01:54.319935: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3 / 30, Time Stamp: 11808.985461235046\n",
      "\ttrain loss: 0.34589430689811707\taccuracy: 0.522778332233429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 20:15:12.750700: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvalidation loss: 0.34610289335250854\taccuracy: 0.515869140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 20:15:13.058073: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4 / 30, Time Stamp: 16207.717448234558\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "\n",
    "def check_accuracy(dataset):\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    for x, y in dataset:\n",
    "        y = tf.one_hot(tf.cast(y, tf.int32), 2)\n",
    "\n",
    "        y_pred = vectorize_layer(x)\n",
    "        # Do not filter when testing.\n",
    "        for i, l in enumerate(layers):\n",
    "            y_pred = l(y_pred)\n",
    "        \n",
    "        loss = tf.reduce_mean(loss_fn(y, y_pred))\n",
    "\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1)), tf.float32\n",
    "            )\n",
    "        )\n",
    "        average_loss += loss\n",
    "        average_accuracy += accuracy\n",
    "    average_loss /= len(dataset)\n",
    "    average_accuracy /= len(dataset)\n",
    "\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "\n",
    "# Set up tensorboard logging.\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = os.path.abspath(\"\") + \"/tflogs/sentiment-%s\" % stamp\n",
    "print(f\"To start tensorboard, run: tensorboard --logdir ./ --host 0.0.0.0\")\n",
    "print(f\"\\ttensorboard profiling requires: pip install tensorboard_plugin_profile\")\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# Initial accuracy\n",
    "loss, accuracy = check_accuracy(val_data)\n",
    "tf.print(f\"\\tvalidation loss: {loss}\\taccuracy: {accuracy}\")\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "for step, (x_batch, y_batch) in enumerate(train_data.take(batch_size)):\n",
    "    print(\n",
    "        f\"Step: {step} / {len(train_data)}, Time Stamp: {time.time() - start_time}\"\n",
    "    )\n",
    "\n",
    "    y_batch = tf.one_hot(tf.cast(y_batch, tf.int32), 2)\n",
    "\n",
    "    if step == 0:\n",
    "        tf.summary.trace_on(graph=True, profiler=True, profiler_outdir=logdir)\n",
    "\n",
    "    x_batch = vectorize_layer(x_batch)  # No shape inference, do outside tf.function\n",
    "    train_step_wrapper(x_batch, y_batch)\n",
    "\n",
    "    # tf.print(\"embedding layer slot counter:\")\n",
    "    # tf.print(embedding_layer._last_slot_count, summarize=-1)\n",
    "    # tf.print(\"embedding layer max slot counter:\")\n",
    "    # tf.print(tf.reduce_max(embedding_layer._last_slot_count), summarize=-1)\n",
    "\n",
    "    if step == 0:\n",
    "        with writer.as_default():\n",
    "            tf.summary.trace_export(name=\"sentiment\", step=step)\n",
    "\n",
    "    loss, accuracy = check_accuracy(train_data)\n",
    "    tf.print(f\"\\ttrain loss: {loss}\\taccuracy: {accuracy}\")\n",
    "    loss, accuracy = check_accuracy(val_data)\n",
    "    tf.print(f\"\\tvalidation loss: {loss}\\taccuracy: {accuracy}\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", loss, step=step)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy, step=step)\n",
    "\n",
    "\n",
    "print(f\"Total training time: {time.time() - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
