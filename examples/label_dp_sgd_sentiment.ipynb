{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB dataset\n",
    "\n",
    "This notebook walks through how perform sentament analysis on the IMDB dataset.\n",
    "In this setting, one party has the reviews and the other party has the labels.\n",
    "The party with the labels is helping the party with the reviews train a model\n",
    "without sharing the labels themselves.\n",
    "\n",
    "Before starting, install tf-shell and the dataset.\n",
    "\n",
    "```bash\n",
    "pip install tf-shell\n",
    "pip install tensorflow_hub tensorflow_datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 05:53:52.002235: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-13 05:53:52.025598: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tf_shell\n",
    "import tf_shell_ml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters for the SHELL encryption library.\n",
    "context = tf_shell.create_context64(\n",
    "    log_n=12,\n",
    "    main_moduli=[288230376151760897, 288230376152137729],\n",
    "    plaintext_modulus=4294991873,\n",
    "    scaling_factor=3,\n",
    "    seed=\"test_seed\",\n",
    ")\n",
    "\n",
    "# Create the secret key for encryption and a rotation key (rotation key is\n",
    "# an auxilary key required for operations like roll or matmul).\n",
    "secret_key = tf_shell.create_key64(context)\n",
    "public_rotation_key = tf_shell.create_rotation_key64(context, secret_key)\n",
    "\n",
    "# The batch size is determined by the ciphertext parameters, specifically the\n",
    "# schemes polynomial's ring degree because tf-shell uses batch axis packing.\n",
    "# Furthermore, two micro-batches to run in parallel.\n",
    "batch_size = context.num_slots\n",
    "\n",
    "use_encryption = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 05:54:11.598078: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /home/vscode/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/tf-shell/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Size...: 100%|██████████| 80/80 [00:04<00:00, 18.44 MiB/s]rl]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:04<00:00,  4.34s/ url]\n",
      "2024-09-13 05:54:37.119837: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/vscode/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "Label: 0\n",
      "Most used words: ['', '[UNK]', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'it']\n",
      "Dictionary size: 10000\n",
      "Word 0 () count: 190260176\n",
      "Word 1 ([UNK]) count: 1908048\n",
      "Word 2 (the) count: 1640589\n",
      "Word 3 (a) count: 793367\n",
      "Word 4 (and) count: 792937\n",
      "Word 5 (of) count: 713899\n",
      "Word 6 (to) count: 663159\n",
      "Word 7 (is) count: 522180\n",
      "Word 8 (in) count: 454454\n",
      "Word 9 (it) count: 379058\n",
      "Word 10 (i) count: 376094\n",
      "Word 11 (this) count: 372402\n",
      "Word 12 (that) count: 342344\n",
      "Word 13 (br) count: 280608\n",
      "Word 14 (was) count: 237739\n",
      "Word 15 (as) count: 227240\n",
      "Word 16 (with) count: 215115\n",
      "Word 17 (for) count: 215019\n",
      "Word 18 (movie) count: 205242\n",
      "Word 19 (but) count: 204007\n",
      "Word 20 (film) count: 184563\n",
      "Word 21 (on) count: 165692\n",
      "Word 22 (not) count: 147470\n",
      "Word 23 (you) count: 146530\n",
      "Word 24 (are) count: 144606\n",
      "Word 25 (his) count: 144318\n",
      "Word 26 (have) count: 137392\n",
      "Word 27 (be) count: 130623\n",
      "Word 28 (he) count: 129926\n",
      "Word 29 (one) count: 125435\n",
      "Word 30 (its) count: 122657\n",
      "Word 31 (all) count: 115104\n",
      "Word 32 (at) count: 114646\n",
      "Word 33 (by) count: 109158\n",
      "Word 34 (they) count: 104446\n",
      "Word 35 (an) count: 104373\n",
      "Word 36 (who) count: 99613\n",
      "Word 37 (from) count: 98877\n",
      "Word 38 (so) count: 97604\n",
      "Word 39 (like) count: 97158\n",
      "Word 40 (or) count: 87118\n",
      "Word 41 (her) count: 86980\n",
      "Word 42 (just) count: 86533\n",
      "Word 43 (about) count: 84370\n",
      "Word 44 (if) count: 83054\n",
      "Word 45 (out) count: 80859\n",
      "Word 46 (has) count: 80649\n",
      "Word 47 (some) count: 78007\n",
      "Word 48 (there) count: 75396\n",
      "Word 49 (what) count: 75491\n",
      "Word 50 (good) count: 71475\n",
      "Word 51 (very) count: 68814\n",
      "Word 52 (when) count: 68908\n",
      "Word 53 (more) count: 68895\n",
      "Word 54 (my) count: 61713\n",
      "Word 55 (even) count: 61146\n",
      "Word 56 (she) count: 60811\n",
      "Word 57 (would) count: 60268\n",
      "Word 58 (no) count: 59316\n",
      "Word 59 (up) count: 58955\n",
      "Word 60 (time) count: 58343\n",
      "Word 61 (really) count: 57514\n",
      "Word 62 (only) count: 57442\n",
      "Word 63 (which) count: 57578\n",
      "Word 64 (had) count: 55691\n",
      "Word 65 (see) count: 55699\n",
      "Word 66 (were) count: 55339\n",
      "Word 67 (their) count: 55382\n",
      "Word 68 (story) count: 54809\n",
      "Word 69 (can) count: 54071\n",
      "Word 70 (me) count: 51803\n",
      "Word 71 (we) count: 48530\n",
      "Word 72 (than) count: 48370\n",
      "Word 73 (much) count: 46081\n",
      "Word 74 (well) count: 45461\n",
      "Word 75 (been) count: 45473\n",
      "Word 76 (get) count: 45216\n",
      "Word 77 (do) count: 45029\n",
      "Word 78 (will) count: 44979\n",
      "Word 79 (also) count: 44631\n",
      "Word 80 (bad) count: 44767\n",
      "Word 81 (because) count: 44384\n",
      "Word 82 (people) count: 44541\n",
      "Word 83 (into) count: 44172\n",
      "Word 84 (other) count: 44230\n",
      "Word 85 (great) count: 43514\n",
      "Word 86 (first) count: 43337\n",
      "Word 87 (how) count: 43293\n",
      "Word 88 (dont) count: 41866\n",
      "Word 89 (most) count: 41685\n",
      "Word 90 (him) count: 41019\n",
      "Word 91 (then) count: 39080\n",
      "Word 92 (movies) count: 38802\n",
      "Word 93 (make) count: 38682\n",
      "Word 94 (made) count: 38338\n",
      "Word 95 (them) count: 38339\n",
      "Word 96 (films) count: 38100\n",
      "Word 97 (any) count: 37665\n",
      "Word 98 (way) count: 37769\n",
      "Word 99 (could) count: 37505\n",
      "Word 100 (too) count: 37306\n",
      "Word 101 (after) count: 36375\n",
      "Word 102 (characters) count: 35590\n",
      "Word 103 (think) count: 35321\n",
      "Word 104 (watch) count: 34037\n",
      "Word 105 (two) count: 32776\n",
      "Word 106 (many) count: 32476\n",
      "Word 107 (being) count: 32295\n",
      "Word 108 (seen) count: 31988\n",
      "Word 109 (character) count: 31853\n",
      "Word 110 (never) count: 31977\n",
      "Word 111 (plot) count: 31203\n",
      "Word 112 (acting) count: 30877\n",
      "Word 113 (best) count: 30728\n",
      "Word 114 (did) count: 30577\n",
      "Word 115 (love) count: 30399\n",
      "Word 116 (little) count: 30455\n",
      "Word 117 (where) count: 30332\n",
      "Word 118 (life) count: 29390\n",
      "Word 119 (show) count: 29199\n",
      "Word 120 (know) count: 28775\n",
      "Word 121 (ever) count: 28504\n",
      "Word 122 (does) count: 28508\n",
      "Word 123 (your) count: 28391\n",
      "Word 124 (still) count: 27215\n",
      "Word 125 (over) count: 27261\n",
      "Word 126 (better) count: 27114\n",
      "Word 127 (these) count: 26639\n",
      "Word 128 (while) count: 26374\n",
      "Word 129 (say) count: 26345\n",
      "Word 130 (off) count: 25916\n",
      "Word 131 (end) count: 25828\n",
      "Word 132 (man) count: 25823\n",
      "Word 133 (scene) count: 25368\n",
      "Word 134 (here) count: 24971\n",
      "Word 135 (such) count: 24965\n",
      "Word 136 (go) count: 24883\n",
      "Word 137 (scenes) count: 24693\n",
      "Word 138 (why) count: 24798\n",
      "Word 139 (through) count: 24340\n",
      "Word 140 (should) count: 24113\n",
      "Word 141 (something) count: 24027\n",
      "Word 142 (im) count: 23657\n",
      "Word 143 (back) count: 23500\n",
      "Word 144 (doesnt) count: 22745\n",
      "Word 145 (those) count: 22692\n",
      "Word 146 (real) count: 22546\n",
      "Word 147 (watching) count: 22416\n",
      "Word 148 (thing) count: 22294\n",
      "Word 149 (years) count: 21953\n",
      "Word 150 (now) count: 21693\n",
      "Word 151 (didnt) count: 21447\n",
      "Word 152 (though) count: 21377\n",
      "Word 153 (actors) count: 20909\n",
      "Word 154 (find) count: 20483\n",
      "Word 155 (nothing) count: 20554\n",
      "Word 156 (actually) count: 20540\n",
      "Word 157 (makes) count: 20544\n",
      "Word 158 (new) count: 20233\n",
      "Word 159 (work) count: 20396\n",
      "Word 160 (before) count: 20393\n",
      "Word 161 (old) count: 20295\n",
      "Word 162 (another) count: 20287\n",
      "Word 163 (going) count: 20187\n",
      "Word 164 (funny) count: 19953\n",
      "Word 165 (every) count: 20038\n",
      "Word 166 (same) count: 20035\n",
      "Word 167 (look) count: 19611\n",
      "Word 168 (few) count: 19627\n",
      "Word 169 (us) count: 19516\n",
      "Word 170 (lot) count: 19084\n",
      "Word 171 (part) count: 18994\n",
      "Word 172 (director) count: 18975\n",
      "Word 173 (again) count: 18872\n",
      "Word 174 (cant) count: 18748\n",
      "Word 175 (quite) count: 18581\n",
      "Word 176 (cast) count: 18326\n",
      "Word 177 (thats) count: 18250\n",
      "Word 178 (want) count: 17881\n",
      "Word 179 (pretty) count: 17967\n",
      "Word 180 (seems) count: 17569\n",
      "Word 181 (things) count: 17432\n",
      "Word 182 (got) count: 17428\n",
      "Word 183 (young) count: 17347\n",
      "Word 184 (around) count: 17212\n",
      "Word 185 (fact) count: 17087\n",
      "Word 186 (enough) count: 16979\n",
      "Word 187 (down) count: 16947\n",
      "Word 188 (however) count: 16845\n",
      "Word 189 (take) count: 16766\n",
      "Word 190 (thought) count: 16623\n",
      "Word 191 (may) count: 16721\n",
      "Word 192 (world) count: 16394\n",
      "Word 193 (both) count: 16363\n",
      "Word 194 (between) count: 16361\n",
      "Word 195 (own) count: 16273\n",
      "Word 196 (give) count: 16183\n",
      "Word 197 (series) count: 16077\n",
      "Word 198 (original) count: 16276\n",
      "Word 199 (ive) count: 15961\n"
     ]
    }
   ],
   "source": [
    "# Split the training set into 60% and 40% to end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_data, val_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)\n",
    "\n",
    "# Print the first example.\n",
    "for review, label in train_data.take(1):\n",
    "    print(\"Review:\", review.numpy().decode('utf-8'))\n",
    "    print(\"Label:\", label.numpy())\n",
    "\n",
    "epochs = 10\n",
    "train_data = train_data.shuffle(buffer_size=2048).batch(batch_size, drop_remainder=True).repeat(count=epochs)\n",
    "val_data = val_data.shuffle(buffer_size=2048).batch(batch_size, drop_remainder=True)\n",
    "test_data = test_data.shuffle(buffer_size=2048).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "vocab_size = 10000  # This dataset has 92061 unique words.\n",
    "max_length = 250\n",
    "embedding_dim = 16\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(train_data.map(lambda text, label: text))\n",
    "\n",
    "print(\"Most used words:\", vectorize_layer.get_vocabulary()[:10])\n",
    "print(\"Dictionary size:\", len(vectorize_layer.get_vocabulary()))\n",
    "\n",
    "# Count the top n words in the training set.\n",
    "top_n = 200\n",
    "word_counts = np.zeros(top_n, dtype=np.int64)\n",
    "for review, label in train_data:\n",
    "    vectorized_reviews = vectorize_layer(review)\n",
    "    for i in range(len(word_counts)):\n",
    "        counts = tf.where(vectorized_reviews == i, 1, 0)\n",
    "        word_counts[i] += tf.reduce_sum(tf.cast(counts, dtype=tf.int64))\n",
    "\n",
    "for i in range(len(word_counts)):\n",
    "    print(f\"Word {i} ({vectorize_layer.get_vocabulary()[i]}) count: {word_counts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainable layers.\n",
    "embedding_layer = tf_shell_ml.ShellEmbedding(\n",
    "    vocab_size + 1,  # +1 for OOV token.\n",
    "    embedding_dim,\n",
    "    skip_embeddings_below_index=top_n,\n",
    ")\n",
    "# TODO dropout layer?\n",
    "hidden_layer = tf_shell_ml.GlobalAveragePooling1D()\n",
    "# TODO dropout layer?\n",
    "output_layer = tf_shell_ml.ShellDense(\n",
    "    2,\n",
    "    activation=tf.nn.softmax,\n",
    ")\n",
    "\n",
    "layers = [\n",
    "    embedding_layer,\n",
    "    hidden_layer,\n",
    "    output_layer,\n",
    "]\n",
    "\n",
    "loss_fn = tf_shell_ml.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `train_step` function which will be called for each batch on an\n",
    "encrypted batch of labels, y. The function first does a forward on the plaintext\n",
    "image x to compute a predicted label, then does backpropagation using the\n",
    "encrypted label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, enc_y):\n",
    "    # Forward pass always in plaintext\n",
    "    y_pred = x\n",
    "    for i, l in enumerate(layers):\n",
    "        y_pred = l(y_pred, training=True)\n",
    "\n",
    "    # Backward pass.\n",
    "    dx = loss_fn.grad(enc_y, y_pred)\n",
    "    dJ_dw = []\n",
    "    dJ_dx = [dx,]\n",
    "    for l in reversed(layers):\n",
    "        if isinstance(l, tf_shell_ml.GlobalAveragePooling1D):\n",
    "            dw, dx = l.backward(dJ_dx[-1])\n",
    "        else:\n",
    "            dw, dx = l.backward(dJ_dx[-1], public_rotation_key)\n",
    "        dJ_dw.extend(dw)\n",
    "        dJ_dx.append(dx)\n",
    "\n",
    "    return reversed(dJ_dw)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_wrapper(x_batch, y_batch):\n",
    "    if use_encryption:\n",
    "        # Encrypt the batch of secret labels y.\n",
    "        enc_y_batch = tf_shell.to_encrypted(y_batch, secret_key, context)\n",
    "    else:\n",
    "        enc_y_batch = y_batch\n",
    "\n",
    "    # Run the training step. The top and bottom halves of the batch are\n",
    "    # treated as two separate mini-batches run in parallel.\n",
    "    enc_grads = train_step(x_batch, enc_y_batch)\n",
    "\n",
    "    filtered_layers = [l for l in layers if len(l.weights) > 0]\n",
    "\n",
    "    if use_encryption:\n",
    "        # Decrypt the weight gradients. In practice, the gradients should be\n",
    "        # noised before decrypting.\n",
    "        packed_grads = [tf_shell.to_tensorflow(g, secret_key) for g in enc_grads]\n",
    "        # Unpack the plaintext gradients using the corresponding layer.\n",
    "        grads = [l.unpack(g) for l, g in zip (filtered_layers, packed_grads)]\n",
    "    else:\n",
    "        grads = enc_grads\n",
    "\n",
    "    weights = []\n",
    "    for l in filtered_layers:\n",
    "        weights+=l.weights\n",
    "\n",
    "    # Apply the gradients to the model.\n",
    "    optimizer.apply_gradients(zip(grads, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training loop. Each inner iteration runs two batches of size\n",
    "$2^{12-1}$ simultaneously.\n",
    "\n",
    "Tensorboard can be used to visualize the training progress. See cell output for\n",
    "command to start tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To start tensorboard, run: tensorboard --logdir ./ --host 0.0.0.0\n",
      "\ttensorboard profiling requires: pip install tensorboard_plugin_profile\n",
      "\tvalidation loss: 0.34678205847740173\taccuracy: 0.50244140625\n",
      "Step: 0 / 30, Time Stamp: 0.4297327995300293\n",
      "WARNING:tensorflow:From /workspaces/tf-shell/.venv/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1369: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.start` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /workspaces/tf-shell/.venv/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1369: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.start` instead.\n",
      "2024-09-13 05:55:43.043938: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-09-13 05:55:43.043966: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /workspaces/tf-shell/.venv/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1420: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /workspaces/tf-shell/.venv/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1420: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /workspaces/tf-shell/.venv/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1420: save (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 07:05:45.882663: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-09-13 07:05:45.932257: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "WARNING:tensorflow:From /workspaces/tf-shell/.venv/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1420: save (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /workspaces/tf-shell/.venv/lib/python3.10/site-packages/tensorflow/python/eager/profiler.py:150: maybe_create_event_file (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /workspaces/tf-shell/.venv/lib/python3.10/site-packages/tensorflow/python/eager/profiler.py:150: maybe_create_event_file (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain loss: 0.3463047742843628\taccuracy: 0.508227527141571\n",
      "\tvalidation loss: 0.3464067280292511\taccuracy: 0.5057373046875\n",
      "Step: 1 / 30, Time Stamp: 4207.950785398483\n",
      "\ttrain loss: 0.3458217680454254\taccuracy: 0.519848644733429\n",
      "\tvalidation loss: 0.34607988595962524\taccuracy: 0.5130615234375\n",
      "Step: 2 / 30, Time Stamp: 8758.485680818558\n",
      "\ttrain loss: 0.34537193179130554\taccuracy: 0.5295491814613342\n",
      "\tvalidation loss: 0.3455347418785095\taccuracy: 0.5269775390625\n",
      "Step: 3 / 30, Time Stamp: 12703.130770683289\n",
      "\ttrain loss: 0.3447962999343872\taccuracy: 0.5438232421875\n",
      "\tvalidation loss: 0.34524139761924744\taccuracy: 0.5352783203125\n",
      "Step: 4 / 30, Time Stamp: 17270.923393011093\n",
      "\ttrain loss: 0.3442564010620117\taccuracy: 0.5548909306526184\n",
      "\tvalidation loss: 0.3448839783668518\taccuracy: 0.5419921875\n",
      "Step: 5 / 30, Time Stamp: 21087.202694416046\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function train_step_wrapper at 0x7f92ab580d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function train_step_wrapper at 0x7f92ab580d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain loss: 0.34387531876564026\taccuracy: 0.5640299320220947\n",
      "\tvalidation loss: 0.34451237320899963\taccuracy: 0.5521240234375\n",
      "Step: 6 / 30, Time Stamp: 24922.99744272232\n",
      "\ttrain loss: 0.34310781955718994\taccuracy: 0.5768473148345947\n",
      "\tvalidation loss: 0.3439071774482727\taccuracy: 0.561767578125\n",
      "Step: 7 / 30, Time Stamp: 29491.1250500679\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "\n",
    "def check_accuracy(dataset):\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    for x, y in dataset:\n",
    "        y = tf.one_hot(tf.cast(y, tf.int32), 2)\n",
    "\n",
    "        y_pred = vectorize_layer(x)\n",
    "        # Do not filter when testing.\n",
    "        for i, l in enumerate(layers):\n",
    "            y_pred = l(y_pred)\n",
    "\n",
    "        loss = tf.reduce_mean(loss_fn(y, y_pred))\n",
    "\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1)), tf.float32\n",
    "            )\n",
    "        )\n",
    "        average_loss += loss\n",
    "        average_accuracy += accuracy\n",
    "    average_loss /= len(dataset)\n",
    "    average_accuracy /= len(dataset)\n",
    "\n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "\n",
    "# Set up tensorboard logging.\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = os.path.abspath(\"\") + \"/tflogs/sentiment-%s\" % stamp\n",
    "print(f\"To start tensorboard, run: tensorboard --logdir ./ --host 0.0.0.0\")\n",
    "print(f\"\\ttensorboard profiling requires: pip install tensorboard_plugin_profile\")\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# Initial accuracy\n",
    "loss, accuracy = check_accuracy(val_data)\n",
    "tf.print(f\"\\tvalidation loss: {loss}\\taccuracy: {accuracy}\")\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "for step, (x_batch, y_batch) in enumerate(train_data.take(batch_size)):\n",
    "    print(f\"Step: {step} / {len(train_data)}, Time Stamp: {time.time() - start_time}\")\n",
    "\n",
    "    y_batch = tf.one_hot(tf.cast(y_batch, tf.int32), 2)\n",
    "\n",
    "    if step == 0:\n",
    "        tf.summary.trace_on(\n",
    "            graph=True,\n",
    "            profiler=True,\n",
    "            # profiler_outdir=logdir,  # Only for tf 2.16+\n",
    "        )\n",
    "\n",
    "    x_batch = vectorize_layer(x_batch)  # No shape inference, do outside tf.function\n",
    "    train_step_wrapper(x_batch, y_batch)\n",
    "\n",
    "    # tf.print(\"embedding layer slot counter:\")\n",
    "    # tf.print(embedding_layer._last_slot_count, summarize=-1)\n",
    "    # tf.print(\"embedding layer max slot counter:\")\n",
    "    # tf.print(tf.reduce_max(embedding_layer._last_slot_count), summarize=-1)\n",
    "\n",
    "    if step == 0:\n",
    "        with writer.as_default():\n",
    "            tf.summary.trace_export(\n",
    "                name=\"sentiment\",\n",
    "                step=step,\n",
    "                profiler_outdir=logdir,\n",
    "            )\n",
    "\n",
    "    loss, accuracy = check_accuracy(train_data)\n",
    "    tf.print(f\"\\ttrain loss: {loss}\\taccuracy: {accuracy}\")\n",
    "    loss, accuracy = check_accuracy(val_data)\n",
    "    tf.print(f\"\\tvalidation loss: {loss}\\taccuracy: {accuracy}\")\n",
    "\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", loss, step=step)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy, step=step)\n",
    "\n",
    "\n",
    "print(f\"Total training time: {time.time() - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
