{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label DP SGD (Post Scale)\n",
    "\n",
    "This notebook walks through how to train a model to recognize hand written\n",
    "digits using label differentially private gradient decent and the MNIST dataset.\n",
    "In this setting, one party has the images and the other party has the labels.\n",
    "They would like to collaborate to train a model without revealing their data.\n",
    "\n",
    "This colab uses the post-scale approach to training.\n",
    "\n",
    "Before starting, install the tf-shell package.\n",
    "\n",
    "```bash\n",
    "pip install tf-shell\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import some modules and set up tf-shell. The parameters are for the SHELL\n",
    "encryption library, which tf-shell uses, and mostly depend on the multiplicative\n",
    "depth of the computation to be performed. This example performs back\n",
    "propagation, thus the multiplicative depth is determined by the number of\n",
    "layers. For more information, see [SHELL](https://github.com/google/shell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 03:16:15.442044: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-08 03:16:15.593868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import tf_shell\n",
    "import tf_shell_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num plaintext bits: 19, noise bits: 40\n",
    "# Max representable value: 61895\n",
    "context = tf_shell.create_context64(\n",
    "    log_n=11,\n",
    "    main_moduli=[576460752303439873],\n",
    "    plaintext_modulus=557057,\n",
    "    scaling_factor=3,\n",
    "    mul_depth_supported=1,\n",
    ")\n",
    "# 121 bits of security according to lattice estimator primal_bdd.\n",
    "\n",
    "# Create the secret key for encryption and a rotation key (rotation key is\n",
    "# an auxilary key required for operations like roll or matmul).\n",
    "secret_key = tf_shell.create_key64(context)\n",
    "public_rotation_key = tf_shell.create_rotation_key64(context, secret_key)\n",
    "\n",
    "# The batch size is determined by the ciphertext parameters, specifically the\n",
    "# schemes polynomial's ring degree because tf-shell uses batch axis packing.\n",
    "# Furthermore, two micro-batches to run in parallel.\n",
    "batch_size = context.num_slots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = np.reshape(x_train, (-1, 784)), np.reshape(x_test, (-1, 784))\n",
    "x_train, x_test = x_train / np.float32(255.0), x_test / np.float32(255.0)\n",
    "y_train, y_test = tf.one_hot(y_train, 10), tf.one_hot(y_test, 10)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2048).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple model with a hidden layer of size 64 and an output layer\n",
    "of size 10 (for each of the 10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_layers = [\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"sigmoid\"),\n",
    "]\n",
    "\n",
    "model = keras.Sequential(mnist_layers)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `train_step` function which will be called for each batch on an\n",
    "encrypted batch of labels, y. The function first does a forward on the plaintext\n",
    "image x to compute a predicted label, then does backpropagation using the\n",
    "encrypted label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    \"\"\"One step of training with using the \"post scale\" approach.\n",
    "\n",
    "    High level idea:\n",
    "    For each output class, backprop to compute the gradient but exclude the loss\n",
    "    function. Now we have a _vector_ of model updates for one sample. The real\n",
    "    gradient update for the sample is a linear combination of the vector of\n",
    "    weight updates whose scale is determined by dJ_dyhat (the derivative of the\n",
    "    loss with respect to the predicted output yhat). Effectively, we have\n",
    "    factored out dJ_dyhat from the gradient. Separating out dJ_dyhat allows us\n",
    "    to scale the weight updates easily when the label is secret and the gradient\n",
    "    must be computed under encryption / multiparty computation because the\n",
    "    multiplicative depth of the computation is 1, however the number of\n",
    "    multiplications required now depends on the model size AND the number of\n",
    "    output classes. In contrast, standard backpropagation only requires\n",
    "    multiplications proportional to the model size, howver the multiplicative\n",
    "    depth is proportional to the model depth.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unset the activation function for the last layer so it is not used in\n",
    "    # computing the gradient. The effect of the last layer activation function\n",
    "    # is factored out of the gradient computation and accounted for below.\n",
    "    model.layers[-1].activation = tf.keras.activations.linear\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)  # forward pass\n",
    "    grads = tape.jacobian(y_pred, model.trainable_weights)\n",
    "    # ^  layers list x (batch size x num output classes x weights) matrix\n",
    "    # dy_pred_j/dW_sample_class\n",
    "\n",
    "\n",
    "    # Reset the activation function for the last layer and compute the real\n",
    "    # prediction.\n",
    "    model.layers[-1].activation = tf.keras.activations.sigmoid\n",
    "    y_pred = model(x, training=False)\n",
    "\n",
    "    # Compute y_pred - y (where y is encrypted).\n",
    "    scalars = y.__rsub__(y_pred)  # dJ/dy_pred\n",
    "    # ^  batch_size x num output classes.\n",
    "\n",
    "    # Expand the last dim so that the subsequent multiplication is\n",
    "    # broadcasted.\n",
    "    scalars = tf_shell.expand_dims(scalars, axis=-1)\n",
    "    # ^ batch_size x num output classes x 1\n",
    "\n",
    "    # Scale each gradient. Since 'scalars' may be a vector of ciphertexts, this\n",
    "    # requires multiplying plaintext gradient for the specific layer (2d) by the\n",
    "    # ciphertext (scalar). To do so efficiently under encryption requires\n",
    "    # flattening and packing the weights, as shown below.\n",
    "    ps_grads = []\n",
    "    for layer_grad_full in grads:\n",
    "        # Remember the original shape of the gradient in order to unpack them\n",
    "        # after the multiplication so they can be applied to the model.\n",
    "        batch_sz = layer_grad_full.shape[0]\n",
    "        num_output_classes = layer_grad_full.shape[1]\n",
    "        grad_shape = layer_grad_full.shape[2:]\n",
    "\n",
    "        packable_grad = tf.reshape(layer_grad_full, [batch_sz, num_output_classes, -1])\n",
    "        # ^  batch_size x num output classes x flattened weights\n",
    "\n",
    "        # Scale the gradient precursors.\n",
    "        scaled_grad = scalars * packable_grad\n",
    "        # ^ dJ/dW = dJ/dy_pred * dy_pred/dW \n",
    "\n",
    "        # Sum over the output classes.\n",
    "        scaled_grad = tf_shell.reduce_sum(scaled_grad, axis=1)\n",
    "        # ^  batch_size x 1 x flattened weights\n",
    "\n",
    "        # In the real world, this approach would also likely require clipping\n",
    "        # the gradient, aggregation, and adding DP noise.\n",
    "\n",
    "        # Reshape to remove the '1' dimension in the middle.\n",
    "        scaled_grad = tf_shell.reshape(scaled_grad, [batch_sz] + grad_shape)\n",
    "        # ^  batch_size x weights\n",
    "\n",
    "        # Sum over the batch.\n",
    "        scaled_grad = tf_shell.reduce_sum(scaled_grad, axis=0, rotation_key=public_rotation_key)\n",
    "        # ^  batch_size x flattened weights\n",
    "        # Every [i, ...] is the same, the sum over the batching dim axis=0.\n",
    "\n",
    "        ps_grads.append(scaled_grad)\n",
    "\n",
    "    return ps_grads\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_wrapper(x_batch, y_batch):\n",
    "    # Encrypt\n",
    "    enc_y_batch = tf_shell.to_encrypted(y_batch, secret_key, context)\n",
    "\n",
    "    # Train\n",
    "    ps_grads = train_step(x_batch, enc_y_batch)\n",
    "\n",
    "    # Decrypt\n",
    "    grads = []\n",
    "    for enc_g in ps_grads:\n",
    "        grads.append(tf_shell.to_tensorflow(enc_g, secret_key)[0])\n",
    "        # ^ take the first element because the grad sum is repeated over the batching dim.\n",
    "\n",
    "    model.optimizer.apply_gradients(\n",
    "        zip(\n",
    "            grads,\n",
    "            model.trainable_weights\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training loop. Each inner iteration runs a batch of size 2^(11),\n",
    "then meaures the model accuracy.\n",
    "\n",
    "Tensorboard can be used to visualize the training progress. See cell output for\n",
    "command to start tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To start tensorboard, run: tensorboard --logdir /tmp/tflogs\n",
      "\n",
      "Start of epoch 0\n",
      "Epoch: 0, Batch: 0 / 30, Time Stamp: 0.07824254035949707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 03:16:18.337431: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-05-08 03:16:18.337456: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error while stopping profiler: Cannot export profiling results. No profiler is running.\n",
      "\taccuracy: 0.13440264761447906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 03:17:43.002417: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-05-08 03:17:43.013809: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-05-08 03:17:43.015068: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: /tmp/tflogs/pt-20240508-031618/plugins/profile/2024_05_08_03_17_43/e81647a0f462.xplane.pb\n",
      "2024-05-08 03:17:43.097148: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 1 / 30, Time Stamp: 84.8675389289856\n",
      "\taccuracy: 0.14435841143131256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 03:18:51.448626: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 2 / 30, Time Stamp: 153.19263339042664\n",
      "\taccuracy: 0.15597344934940338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 03:19:58.580131: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 3 / 30, Time Stamp: 220.32476949691772\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "start_time = time.time()\n",
    "\n",
    "# Set up tensorboard logging.\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"/tmp/tflogs/pt-%s\" % stamp\n",
    "print(f\"To start tensorboard, run: tensorboard --logdir /tmp/tflogs\")\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset.take(batch_size)):\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Batch: {step} / {len(train_dataset)}, Time Stamp: {time.time() - start_time}\"\n",
    "        )\n",
    "\n",
    "        # Skip the last batch if it is not full for performance.\n",
    "        if x_batch.shape[0] != batch_size:\n",
    "            break\n",
    "\n",
    "        # If using deferred execution, one can trace and profile the training.\n",
    "        if step == 0:\n",
    "            tf.summary.trace_on(graph=True, profiler=True, profiler_outdir=logdir)\n",
    "\n",
    "        train_step_wrapper(x_batch, y_batch)\n",
    "\n",
    "        if step == 0:\n",
    "            with writer.as_default():\n",
    "                tf.summary.trace_export(\n",
    "                    name=\"label_dp_sgd_post_scale\", step=(epoch + 1) * step\n",
    "                )\n",
    "\n",
    "        # Check the accuracy.\n",
    "        average_loss = 0\n",
    "        average_accuracy = 0\n",
    "        for x, y in val_dataset:\n",
    "            y_pred = model(x, training=False)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, y_pred))\n",
    "            accuracy = tf.reduce_mean(\n",
    "                tf.cast(\n",
    "                    tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1)), tf.float32\n",
    "                )\n",
    "            )\n",
    "            average_accuracy += accuracy\n",
    "            average_loss += loss\n",
    "        average_loss /= len(val_dataset)\n",
    "        average_accuracy /= len(val_dataset)\n",
    "        tf.print(f\"\\taccuracy: {accuracy}\")\n",
    "\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(\"loss\", average_loss, step=(epoch + 1) * step)\n",
    "            tf.summary.scalar(\n",
    "                \"accuracy\", average_accuracy, step=(epoch + 1) * step\n",
    "            )\n",
    "\n",
    "\n",
    "print(f\"Total training time: {time.time() - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
