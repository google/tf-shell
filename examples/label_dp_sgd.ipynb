{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label DP SGD\n",
    "\n",
    "This notebook walks through how to train a model to recognize hand written\n",
    "digits using label differentially private gradient decent and the MNIST dataset.\n",
    "In this setting, one party has the images and the other party has the labels.\n",
    "They would like to collaborate to train a model without revealing their data.\n",
    "\n",
    "Before starting, install the tf-shell package.\n",
    "\n",
    "```bash\n",
    "pip install tf-shell\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import some modules and set up tf-shell. The parameters are for the SHELL\n",
    "encryption library, which tf-shell uses, and mostly depend on the multiplicative\n",
    "depth of the computation to be performed. This example performs back\n",
    "propagation, thus the multiplicative depth is determined by the number of\n",
    "layers. For more information, see [SHELL](https://github.com/google/shell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:19:38.518051: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-08 01:19:38.539912: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import tf_shell\n",
    "import tf_shell_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters for the SHELL encryption library.\n",
    "context = tf_shell.create_context64(\n",
    "    log_n=12,\n",
    "    main_moduli=[288230376151760897, 288230376152137729],\n",
    "    plaintext_modulus=4294991873,\n",
    "    scaling_factor=3,\n",
    "    mul_depth_supported=3,\n",
    "    seed=\"test_seed\",\n",
    ")\n",
    "\n",
    "# Create the secret key for encryption and a rotation key (rotation key is\n",
    "# an auxilary key required for operations like roll or matmul).\n",
    "secret_key = tf_shell.create_key64(context)\n",
    "public_rotation_key = tf_shell.create_rotation_key64(context, secret_key)\n",
    "\n",
    "# The batch size is determined by the ciphertext parameters, specifically the\n",
    "# schemes polynomial's ring degree because tf-shell uses batch axis packing.\n",
    "# Furthermore, two micro-batches to run in parallel.\n",
    "batch_size = context.num_slots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = np.reshape(x_train, (-1, 784)), np.reshape(x_test, (-1, 784))\n",
    "x_train, x_test = x_train / np.float32(255.0), x_test / np.float32(255.0)\n",
    "y_train, y_test = tf.one_hot(y_train, 10), tf.one_hot(y_test, 10)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2048).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple model with a hidden layer of size 64 and an output layer\n",
    "of size 10 (for each of the 10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the layers\n",
    "hidden_layer = tf_shell_ml.ShellDense(\n",
    "    64,\n",
    "    activation=tf_shell_ml.relu,\n",
    "    activation_deriv=tf_shell_ml.relu_deriv,\n",
    "    is_first_layer=True,\n",
    ")\n",
    "output_layer = tf_shell_ml.ShellDense(\n",
    "    10,\n",
    "    activation=tf.nn.softmax,\n",
    ")\n",
    "\n",
    "# Call the layers once to create the weights.\n",
    "y1 = hidden_layer(tf.zeros((batch_size, 784)))\n",
    "y2 = output_layer(y1)\n",
    "\n",
    "loss_fn = tf_shell_ml.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `train_step` function which will be called for each batch on an\n",
    "encrypted batch of labels, y. The function first does a forward on the plaintext\n",
    "image x to compute a predicted label, then does backpropagation using the\n",
    "encrypted label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, enc_y):\n",
    "    # Forward pass always in plaintext\n",
    "    y_1 = hidden_layer(x)\n",
    "    y_pred = output_layer(y_1)\n",
    "\n",
    "    # Backward pass.\n",
    "    dJ_dy_pred = loss_fn.grad(enc_y, y_pred)\n",
    "    dJ_dw1, dJ_dx1 = output_layer.backward(dJ_dy_pred, public_rotation_key)\n",
    "    dJ_dw0, _ = hidden_layer.backward(dJ_dx1, public_rotation_key)\n",
    "\n",
    "    # dJ_dw1, the output layer gradient, would usually have shape [10] for the\n",
    "    # 10 classes. tf-shell instead back propagates in two mini-batches per batch\n",
    "    # resulting in two gradients of shape [10]. Furthermore, the gradients are\n",
    "    # in an \"expanded\" form where the gradient is repeated by the size of the\n",
    "    # batch. Said another way, if real_grad_top/bottom is the \"real\" gradient of\n",
    "    # shape [10] from the top/bottom halves of the batch:\n",
    "    #\n",
    "    # dJ_dw = tf.concat([\n",
    "    #   tf.repeat(\n",
    "    #       tf.expand_dims(real_grad_top, 0), repeats=[batch_sz // 2], axis=0\n",
    "    #   ),\n",
    "    #   tf.repeat(\n",
    "    #       tf.expand_dims(real_grad_bottom, 0), repeats=[batch_sz // 2], axis=0\n",
    "    #   )\n",
    "    # ])\n",
    "    #\n",
    "    # This repetition is result of the SHELL library using a packed\n",
    "    # representation of ciphertexts for efficiency. As such, if the ciphertexts\n",
    "    # need to be sent over the network, they may be masked and packed together\n",
    "    # before being transmitted to the party with the key.\n",
    "    #\n",
    "    # Only return the weight gradients at [0], not the bias gradients at [1].\n",
    "    # The bias is not used in this test.\n",
    "    return [dJ_dw1[0], dJ_dw0[0]]\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_wrapper(x_batch, y_batch):\n",
    "    x_batch = tf.cast(x_batch, tf.float32)\n",
    "    y_batch = tf.cast(y_batch, tf.float32)\n",
    "\n",
    "    # Encrypt the batch of secret labels y.\n",
    "    enc_y_batch = tf_shell.to_encrypted(y_batch, secret_key, context)\n",
    "\n",
    "    # Run the training step. The top and bottom halves of the batch are\n",
    "    # treated as two separate mini-batches run in parallel to maximize\n",
    "    # efficiency.\n",
    "    enc_grads = train_step(x_batch, enc_y_batch)\n",
    "\n",
    "    # Decrypt the weight gradients. In practice, the gradients should be\n",
    "    # noised before decrypting.\n",
    "    repeated_grads = [tf_shell.to_tensorflow(g, secret_key) for g in enc_grads]\n",
    "\n",
    "    # Pull out grads from the top and bottom batches.\n",
    "    top_grad = [g[0] for g in repeated_grads]\n",
    "    bottom_grad = [g[batch_size // 2] for g in repeated_grads]\n",
    "\n",
    "    # Decrypt the weight gradients. In practice, the gradients should be\n",
    "    # noised before decrypting.\n",
    "    weights = output_layer.weights + hidden_layer.weights\n",
    "\n",
    "    optimizer.apply_gradients(zip(top_grad, weights))\n",
    "    optimizer.apply_gradients(zip(bottom_grad, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training loop. Each inner iteration runs two batches of size\n",
    "$2^{12-1}$ simultaneously.\n",
    "\n",
    "Tensorboard can be used to visualize the training progress. See cell output for\n",
    "command to start tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To start tensorboard, run: tensorboard --logdir /tmp/tflogs\n",
      "\n",
      "Start of epoch 0\n",
      "Epoch: 0, Batch: 0 / 15, Time Stamp: 0.06978559494018555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:19:54.275369: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-05-08 01:19:54.275391: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error while stopping profiler: Cannot export profiling results. No profiler is running.\n",
      "\taccuracy: 0.15154866874217987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:26:14.168977: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-05-08 01:26:14.181516: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-05-08 01:26:14.182392: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: /tmp/tflogs/pt-20240508-011954/plugins/profile/2024_05_08_01_26_14/e81647a0f462.xplane.pb\n",
      "2024-05-08 01:26:14.216694: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 1 / 15, Time Stamp: 380.09435200691223\n",
      "\taccuracy: 0.16648229956626892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:32:34.396483: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 2 / 15, Time Stamp: 760.1941771507263\n",
      "\taccuracy: 0.14712388813495636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:38:54.856343: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 3 / 15, Time Stamp: 1140.6538624763489\n",
      "\taccuracy: 0.16592919826507568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:45:17.714885: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 4 / 15, Time Stamp: 1523.5124411582947\n",
      "\taccuracy: 0.1692477911710739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:51:34.837468: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 5 / 15, Time Stamp: 1900.635261774063\n",
      "\taccuracy: 0.17865043878555298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:57:54.261685: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 6 / 15, Time Stamp: 2280.059217453003\n",
      "\taccuracy: 0.18860618770122528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 02:04:14.045979: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 7 / 15, Time Stamp: 2659.84348654747\n",
      "\taccuracy: 0.1946902722120285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 02:10:38.937992: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 8 / 15, Time Stamp: 3044.7355086803436\n",
      "\taccuracy: 0.20243363082408905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 02:16:59.938804: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 9 / 15, Time Stamp: 3425.7369046211243\n",
      "\taccuracy: 0.22676990926265717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 02:23:25.627886: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 10 / 15, Time Stamp: 3811.425350189209\n",
      "\taccuracy: 0.24834071099758148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 02:29:44.326151: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 11 / 15, Time Stamp: 4190.123619794846\n",
      "\taccuracy: 0.26216813921928406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 02:35:59.882177: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 12 / 15, Time Stamp: 4565.6800808906555\n",
      "\taccuracy: 0.2815265357494354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 02:42:16.677880: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 13 / 15, Time Stamp: 4942.475761651993\n",
      "\taccuracy: 0.28595131635665894\n",
      "Epoch: 0, Batch: 14 / 15, Time Stamp: 5321.51356959343\n",
      "Total training time: 5321.514094829559 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 02:48:35.716055: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "start_time = time.time()\n",
    "\n",
    "# Set up tensorboard logging.\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"/tmp/tflogs/pt-%s\" % stamp\n",
    "print(f\"To start tensorboard, run: tensorboard --logdir /tmp/tflogs\")\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset.take(batch_size)):\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Batch: {step} / {len(train_dataset)}, Time Stamp: {time.time() - start_time}\"\n",
    "        )\n",
    "\n",
    "        # Skip the last batch if it is not full for performance.\n",
    "        if x_batch.shape[0] != batch_size:\n",
    "            break\n",
    "\n",
    "        # If using deferred execution, one can trace and profile the training.\n",
    "        if step == 0:\n",
    "            tf.summary.trace_on(graph=True, profiler=True, profiler_outdir=logdir)\n",
    "\n",
    "        train_step_wrapper(x_batch, y_batch)\n",
    "\n",
    "        if step == 0:\n",
    "            with writer.as_default():\n",
    "                tf.summary.trace_export(\n",
    "                    name=\"label_dp_sgd\", step=(epoch + 1) * step\n",
    "                )\n",
    "\n",
    "        # Check the accuracy.\n",
    "        average_loss = 0\n",
    "        average_accuracy = 0\n",
    "        for x, y in val_dataset:\n",
    "            y_pred = output_layer(hidden_layer(x))\n",
    "            loss = tf.reduce_mean(loss_fn(y, y_pred))\n",
    "            accuracy = tf.reduce_mean(\n",
    "                tf.cast(\n",
    "                    tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1)), tf.float32\n",
    "                )\n",
    "            )\n",
    "            average_loss += loss\n",
    "            average_accuracy += accuracy\n",
    "        average_loss /= len(val_dataset)\n",
    "        average_accuracy /= len(val_dataset)\n",
    "        tf.print(f\"\\taccuracy: {accuracy}\")\n",
    "\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(\"loss\", average_loss, step=(epoch + 1) * batch_size - 1)\n",
    "            tf.summary.scalar(\n",
    "                \"accuracy\", average_accuracy, step=(epoch + 1) * batch_size - 1\n",
    "            )\n",
    "\n",
    "\n",
    "print(f\"Total training time: {time.time() - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
