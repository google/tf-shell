{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label DP SGD\n",
    "\n",
    "This notebook walks through how to train a model to recognize hand written\n",
    "digits using label differentially private gradient decent and the MNIST dataset.\n",
    "\n",
    "Before starting, install the tfshell package.\n",
    "\n",
    "First, import some modules and set up the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import shell_tensor\n",
    "import shell_ml\n",
    "\n",
    "batch_size = 1024 # must match SHELL polynomial degree\n",
    "\n",
    "# Prepare the training dataset.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2048).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up the SHELL encryption library. The choice of security parameters is\n",
    "for testing purposes only and is not secure. The parameters heavily depend on\n",
    "the multiplicative depth of the computation to be performed, which in this case\n",
    "is backpropagation, thus the number of layers has a high impact.\n",
    "\n",
    "Here we create a simple model with a hidden layer of size 64 and an output layer\n",
    "of size 10 (for each of the 10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_params = shell_tensor.shell.ContextParams64(\n",
    "    modulus=shell_tensor.shell.kModulus59,\n",
    "    log_n=10,\n",
    "    log_t=16,\n",
    "    variance=0,  # Too low for prod. Okay for test.\n",
    ")\n",
    "context = shell_tensor.create_context64(ct_params)\n",
    "prng = shell_tensor.create_prng()\n",
    "key = shell_tensor.create_key64(context, prng)\n",
    "\n",
    "hidden_layer = label_dp_sgd.ShellDense(64, activation=label_dp_sgd.relu, activation_deriv=label_dp_sgd.relu_deriv)\n",
    "output_layer = label_dp_sgd.ShellDense(10, activation=label_dp_sgd.sigmoid, activation_deriv=label_dp_sgd.sigmoid_deriv)\n",
    "\n",
    "# Call the layers to create the weights\n",
    "y1 = hidden_layer(tf.zeros((batch_size, 784)))\n",
    "y2 = output_layer(y1)\n",
    "\n",
    "loss_fn = label_dp_sgd.CategoricalCrossentropy()\n",
    "optimizer = label_dp_sgd.Adam()\n",
    "optimizer.compile([hidden_layer.weights, output_layer.weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the custom training loop. The `train_step` function is called for each\n",
    "batch which first encrypts the input y, does a forward pass on the model in\n",
    "plaintext to set up gradient precursors, then does backpropagation under\n",
    "encryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_after_n_batches = 4\n",
    "epochs = 1\n",
    "start_time = time.time()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    # In practice, input y would be quantized to fixed point before encryption.\n",
    "    # This is not done here to reduce dependencies on external libraries.\n",
    "    y = tf.cast(y, tf.int32)\n",
    "\n",
    "    # Encrypt y\n",
    "    y = shell_tensor.to_shell_tensor(context, y).get_encrypted(prng, key)\n",
    "\n",
    "    # Forward pass in plaintext\n",
    "    y_1 = hidden_layer(x)\n",
    "    y_pred = output_layer(y_1)\n",
    "\n",
    "    # Backward pass under encryption\n",
    "    dJ_dy_pred = loss_fn.grad(y, y_pred)\n",
    "    (dJ_dw1, dJ_dx1) = output_layer.backward(dJ_dy_pred, False, prng, key)\n",
    "    (dJ_dw0, dJ_dx0_unused) = hidden_layer.backward(dJ_dx1, True, prng, key)\n",
    "\n",
    "    # In practice, the gradients are likely secret and should be aggregated and\n",
    "    # noised before decrypting and applying to the weights. Furthermore, weight\n",
    "    # gradients are in an \"expanded\" form where each element of a the ciphertext\n",
    "    # polynomial holds the same value, the gradient. What this means is dJ_dw1\n",
    "    # is a tensor of shape [1024, 10] the the real gradient is of shape [10].\n",
    "    # Said another way,\n",
    "    # \n",
    "    # dJ_dw1 = tf.repeat(real_grad, repeats=[1024], axis=0)\n",
    "    #\n",
    "    # This repition may seem wasteful, and it is, but it is product of the\n",
    "    # polynomial representation of ciphertexts. As such, decryption may be more\n",
    "    # efficient if ciphertexts are packed together before being transmitted to\n",
    "    # the party with the key.\n",
    "    dJ_dw1 = dJ_dw1[0].get_decrypted(key)\n",
    "    dJ_dw0 = dJ_dw0[0].get_decrypted(key)\n",
    "\n",
    "    # Decrypt and apply the weight gradients. dJ_dw[1] is bias.\n",
    "    dJ_dw1 = tf.cast(dJ_dw1, tf.float32)\n",
    "    optimizer.grad_to_weight(output_layer.weights, dJ_dw1)\n",
    "\n",
    "    dJ_dw0 = tf.cast(dJ_dw0, tf.float32)\n",
    "    optimizer.grad_to_weight(hidden_layer.weights, dJ_dw0)\n",
    "\n",
    "\n",
    "# Set up tensorboard logging.\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"/tmp/tflogs/pt-%s\" % stamp\n",
    "print(f\"tensorboard --logdir /tmp/tflogs\")\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # Log every 2 batches.\n",
    "        if step % 2 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {step} / {len(train_dataset)}, Time: {time.time() - start_time}\")\n",
    "\n",
    "        train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        if step == stop_after_n_batches:\n",
    "            break\n",
    "\n",
    "print(f\"Total plaintext training time: {time.time() - start_time} seconds\")\n",
    "\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(name=\"my_func_trace\", step=0, profiler_outdir=logdir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
