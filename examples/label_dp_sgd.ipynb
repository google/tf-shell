{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label DP SGD\n",
    "\n",
    "This notebook walks through how to train a model to recognize hand written\n",
    "digits using label differentially private gradient decent and the MNIST dataset.\n",
    "\n",
    "Before starting, install the tf-shell package.\n",
    "\n",
    "```bash\n",
    "pip install tf-shell\n",
    "```\n",
    "\n",
    "First, import some modules and set up tf-shell. These parameters are for the\n",
    "SHELL encryption library, which tf-shell uses. The parameters mostly depend on\n",
    "the multiplicative depth of the computation to be performed, which in this\n",
    "example is back propagation, and thus is mostly set by the number of layers. For\n",
    "more information, see [SHELL](https://github.com/google/shell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 18:26:55.026140: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-02 18:26:55.047894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import shell_tensor\n",
    "import shell_ml\n",
    "\n",
    "# First set up parameters for the SHELL encryption library.\n",
    "log_slots = 11\n",
    "slots = 2**log_slots\n",
    "\n",
    "# Num plaintext bits: 27, noise bits: 65, num rns moduli: 2\n",
    "context = shell_tensor.create_context64(\n",
    "    log_n=11,\n",
    "    main_moduli=[140737488486401, 140737488498689],\n",
    "    aux_moduli=[],\n",
    "    plaintext_modulus=134246401,\n",
    "    noise_variance=8,\n",
    "    seed=\"\",\n",
    ")\n",
    "\n",
    "# Create the secret key and a rotation key for certain operations.\n",
    "key = shell_tensor.create_key64(context)\n",
    "rotation_key = shell_tensor.create_rotation_key64(context, key)\n",
    "\n",
    "# The most efficient batch size is determined by the ciphertext parameters.\n",
    "# The batch_size is set to the ciphertext polynomial's ring degree allowing two\n",
    "# mini-batches to run in parallel.\n",
    "batch_size = slots\n",
    "fxp_num_bits = 8  # number of fractional bits to use in fixed-point encoding.\n",
    "plaintext_dtype = tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = np.reshape(x_train, (-1, 784)), np.reshape(x_test, (-1, 784))\n",
    "x_train, x_test = x_train / np.float32(255.0), x_test / np.float32(255.0)\n",
    "y_train, y_test = tf.one_hot(y_train, 10), tf.one_hot(y_test, 10)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2048).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple model with a hidden layer of size 64 and an output layer\n",
    "of size 10 (for each of the 10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the layers\n",
    "hidden_layer = shell_ml.ShellDense(\n",
    "    64,\n",
    "    activation=shell_ml.relu,\n",
    "    activation_deriv=shell_ml.relu_deriv,\n",
    "    fxp_fractional_bits=fxp_num_bits,\n",
    "    weight_dtype=plaintext_dtype,\n",
    ")\n",
    "output_layer = shell_ml.ShellDense(\n",
    "    10,\n",
    "    fxp_fractional_bits=fxp_num_bits,\n",
    "    weight_dtype=plaintext_dtype,\n",
    ")\n",
    "\n",
    "# Call the layers once to create the weights.\n",
    "y1 = hidden_layer(tf.zeros((batch_size, 784)))\n",
    "y2 = output_layer(y1)\n",
    "\n",
    "loss_fn = shell_ml.CategoricalCrossentropy()\n",
    "optimizer = shell_ml.Adam()\n",
    "optimizer.compile([hidden_layer.weights, output_layer.weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the custom training loop. The `train_step` function is called for each\n",
    "batch which first encrypts the input y, does a forward pass on the model in\n",
    "plaintext to set up gradient precursors, then does backpropagation under\n",
    "encryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir /tmp/tflogs\n",
      "WARNING:tensorflow:Trace already enabled\n",
      "\n",
      "Start of epoch 0\n",
      "Epoch 0 - Accuracy: 0.07300885021686554\n",
      "Total plaintext training time: 0.0222165584564209 seconds\n",
      "WARNING:tensorflow:From /home/vscode/.local/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1431: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:From /home/vscode/.local/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1431: save (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n",
      "WARNING:tensorflow:From /home/vscode/.local/lib/python3.10/site-packages/tensorflow/python/eager/profiler.py:150: maybe_create_event_file (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 21:45:08.784033: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-02-02 21:45:08.886853: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "stop_after_n_batches = 2\n",
    "epochs = 1\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def train_step(x, enc_y):\n",
    "    # Forward pass always in plaintext\n",
    "    y_1 = hidden_layer(x)\n",
    "    y_pred = output_layer(y_1)\n",
    "\n",
    "    # Backward pass.\n",
    "    dJ_dy_pred = loss_fn.grad(enc_y, y_pred)\n",
    "    (dJ_dw1, dJ_dx1) = output_layer.backward(\n",
    "        dJ_dy_pred, rotation_key, is_first_layer=False\n",
    "    )\n",
    "    (dJ_dw0, _) = hidden_layer.backward(dJ_dx1, rotation_key, is_first_layer=True)\n",
    "\n",
    "    # dJ_dw1, the output layer gradient, would usually have shape [10] for the\n",
    "    # 10 classes. tf-shell instead back propagates in two mini-batches per batch\n",
    "    # resulting in two gradients of shape [10]. Furthermore, the gradients are\n",
    "    # in an \"expanded\" form where the gradient is repeated by the size of the\n",
    "    # mini-batch. Said another way, if real_grad_top/bottom is the \"real\"\n",
    "    # gradient of shape [10] from the top/bottom halves of the batch:\n",
    "    #\n",
    "    # dJ_dw = tf.concat([\n",
    "    #   tf.repeat(\n",
    "    #       tf.expand_dims(real_grad_top, 0), repeats=[batch_sz // 2], axis=0\n",
    "    #   ),\n",
    "    #   tf.repeat(\n",
    "    #       tf.expand_dims(real_grad_bottom, 0), repeats=[batch_sz // 2], axis=0\n",
    "    #   )\n",
    "    # ])\n",
    "    #\n",
    "    # This repetition is result of the SHELL library using a \"packed\"\n",
    "    # representation of ciphertexts for efficiency. As such, if the ciphertexts\n",
    "    # need to be sent over the network, they may be masked and packed together\n",
    "    # before being transmitted to the party with the key.\n",
    "    #\n",
    "    # Only return the weight gradients at [0], not the bias gradients at [1].\n",
    "    # The bias is not used in this test.\n",
    "    return dJ_dw1[0], dJ_dw0[0]\n",
    "\n",
    "\n",
    "# Set up tensorboard logging.\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"/tmp/tflogs/pt-%s\" % stamp\n",
    "print(f\"tensorboard --logdir /tmp/tflogs\")\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Check the accuracy.\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    for x, y in val_dataset:\n",
    "        y_pred = output_layer(hidden_layer(x))\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1)), tf.float32))\n",
    "        average_accuracy += accuracy\n",
    "    average_loss /= len(val_dataset)\n",
    "    average_accuracy /= len(val_dataset)\n",
    "    tf.print(f\"Before Training: Epoch {epoch} - Accuracy: {accuracy}\")\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset.take(stop_after_n_batches)):\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Batch: {step} / {len(train_dataset)}, Time Stamp: {time.time() - start_time}\"\n",
    "        )\n",
    "\n",
    "        x_batch = tf.cast(x_batch, tf.float32)\n",
    "        y_batch = tf.cast(y_batch, tf.float32)\n",
    "\n",
    "        # Encrypt the batch of secret labels y.\n",
    "        enc_y_batch = shell_tensor.to_shell_tensor(\n",
    "            context, y_batch, fxp_fractional_bits=fxp_num_bits\n",
    "        ).get_encrypted(key)\n",
    "\n",
    "        # Run the training step. The top and bottom halves of the batch are\n",
    "        # treated as two separate mini-batches run in parallel to maximize\n",
    "        # efficiency.\n",
    "        enc_output_layer_grad, enc_hidden_layer_grad = train_step(x_batch, enc_y_batch)\n",
    "\n",
    "        # Decrypt the weight gradients. In practice, the gradients should be\n",
    "        # noised before decrypting.\n",
    "        repeated_output_layer_grad = enc_output_layer_grad.get_decrypted(key)\n",
    "        repeated_hidden_layer_grad = enc_hidden_layer_grad.get_decrypted(key)\n",
    "\n",
    "        # Apply the gradients to the model. We choose the first dimension at\n",
    "        # index 0 arbitrarily. The weight gradients are repeated across the\n",
    "        # first dimension.\n",
    "        optimizer.grad_to_weight(output_layer.weights, repeated_output_layer_grad[0])\n",
    "        optimizer.grad_to_weight(hidden_layer.weights, repeated_hidden_layer_grad[0])\n",
    "        optimizer.grad_to_weight(\n",
    "            output_layer.weights, repeated_output_layer_grad[batch_size // 2]\n",
    "        )\n",
    "        optimizer.grad_to_weight(\n",
    "            hidden_layer.weights, repeated_hidden_layer_grad[batch_size // 2]\n",
    "        )\n",
    "    \n",
    "    # Check the accuracy.\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    for x, y in val_dataset:\n",
    "        y_pred = output_layer(hidden_layer(x))\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1)), tf.float32))\n",
    "        average_accuracy += accuracy\n",
    "    average_loss /= len(val_dataset)\n",
    "    average_accuracy /= len(val_dataset)\n",
    "    tf.print(f\"After Training: Epoch {epoch} - Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "print(f\"Total plaintext training time: {time.time() - start_time} seconds\")\n",
    "\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(name=\"mnist_shell_example\", step=0, profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
