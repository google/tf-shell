{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tf-shell with Multiple Machines\n",
    "\n",
    "`tf-shell` can be run on multiple machines using TensorFlow device placement.\n",
    "\n",
    "A TensorFlow cluster is set up by each machine running something like the\n",
    "following:\n",
    "\n",
    "```python\n",
    "cluster = tf.train.ClusterSpec('''{\n",
    "  \"alice\": [\"alice.com:2222\"],\n",
    "  \"bob\": [\"bob.com:2223\"],\n",
    "}''')\n",
    "\n",
    "server = tf.distribute.Server(\n",
    "    cluster,\n",
    "    job_name=\"/job:alice/replica:0/task:0/device:CPU:0\",  # or bob\n",
    "    task_index=0,\n",
    ")\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(cluster)\n",
    "```\n",
    "\n",
    "In this notebook, we will emulate distributed execution on a single machine by\n",
    "using the special job name `/job:localhost/replica:0/task:0/device:CPU:0` for\n",
    "both alice and bob, and skip the server setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
    "bob = \"/job:localhost/replica:0/task:0/device:CPU:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `tf-shell` works with sensitive cryptographic material, it is important to\n",
    "tell TensorFlow to only place ops on devices which were explicitly assigned,\n",
    "for security reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 05:37:22.799257: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-30 05:37:22.825891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_soft_device_placement(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow makes it easy to schedule operations on specific parties. In this\n",
    "example, Alice will generate a secret key, encrypt the input x, and send it to\n",
    "Bob. Bob will square the value, and return to alice, who will decrypt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected BGV parameters:\n",
      "log_n: 11\n",
      "t: 12289 (14 bits, min:6)\n",
      "qs: 2251800363651073  (52 bits, min:52 = t:14 + noise:38 + offset:0)\n",
      "INFO: Generating key\n",
      "tf.Tensor(25.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tf_shell\n",
    "\n",
    "param_cache = \"/tmp/example_protocol_param_cache\"\n",
    "\n",
    "@tf.function\n",
    "def example_protocol(x):\n",
    "  with tf.device(alice):\n",
    "    shell_context = tf_shell.create_autocontext64(\n",
    "        log2_cleartext_sz=6,\n",
    "        scaling_factor=1,\n",
    "        noise_offset_log2=0,\n",
    "        cache_path=param_cache,\n",
    "    )\n",
    "    key = tf_shell.create_key64(shell_context, param_cache)\n",
    "\n",
    "    enc_x = tf_shell.to_encrypted(x, key, shell_context)\n",
    "\n",
    "  with tf.device(bob):\n",
    "    enc_x_squared = enc_x * enc_x\n",
    "  \n",
    "  with tf.device(alice):\n",
    "    x_squared = tf_shell.to_tensorflow(enc_x_squared, key)\n",
    "    return x_squared\n",
    "\n",
    "# Turn on shell graph optimizers and deferred execution to use autocontext.\n",
    "tf_shell.enable_optimization()\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "res = example_protocol(tf.constant([5.0]))\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we used the `cache_path` arguments when creating the context\n",
    "and key. This prevents regenerating the parameters every time the code is run.\n",
    "If we call the function again, these parameters will be loaded from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Generating key\n",
      "tf.Tensor(36.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "res = example_protocol(tf.constant([6.0]))\n",
    "print(res[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
