# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import tensorflow as tf
import tf_shell
from tf_shell_ml.private_base import PrivateBase
from tf_shell_ml import large_tensor


class PostScaleModel(PrivateBase):
    def __init__(
        self,
        *args,
        ubatch_per_batch=1,
        jacobian_strategy=tf.distribute.get_strategy(),
        **kwargs,
    ):
        if not isinstance(ubatch_per_batch, int) or ubatch_per_batch <= 0:
            raise ValueError(
                "ubatch_per_batch must be a positive integer, got: "
                f"{ubatch_per_batch}"
            )
        self.ubatch_per_batch = ubatch_per_batch
        self.jacobian_strategy = jacobian_strategy

        super().__init__(*args, **kwargs)

        for l in self.layers:
            if "tf_shell_ml" in getattr(l, "__module__", None):
                raise ValueError(
                    "tf_shell_ml.PostScaleSequential does not support tf_shell layers"
                )

    def call(self, inputs, training=False, with_softmax=True):
        prediction = super().call(inputs, training)
        if with_softmax:
            # Perform the last layer activation since it is removed for training
            # purposes.
            prediction = tf.nn.softmax(prediction)
        return prediction

    def _predict_and_jacobian(self, features):
        """
        Predicts the output for the given features and optionally computes the
        Jacobian.

        Args:
            features (tf.Tensor): Input features for the model.

        Returns:
            tuple: A tuple containing:
                - predictions (tf.Tensor): The model output after applying
                  softmax.
                - jacobians (list or tf.Tensor): The Jacobian of the last layer
                  preactivation with respect to the model weights.
        """
        with tf.GradientTape(
            persistent=tf.executing_eagerly() or self.jacobian_pfor
        ) as tape:
            predictions = self.call(features, training=True, with_softmax=False)

        jacobians = tape.jacobian(
            predictions,
            self.trainable_variables,
            # unconnected_gradients=tf.UnconnectedGradients.ZERO, broken with pfor
            parallel_iterations=self.jacobian_pfor_iterations,
            experimental_use_pfor=self.jacobian_pfor,
        )
        # ^  layers list x shape: [batch size, num output classes, weights]

        # Compute the last layer's activation manually since we skipped it above.
        predictions = tf.nn.softmax(predictions)

        return predictions, jacobians

    def _backward(self, dJ_dz, jacobians):
        # Scale each gradient. Since 'scalars' may be a vector of
        # ciphertexts, this requires multiplying plaintext gradient for the
        # specific layer (2d) by the ciphertext (scalar).
        grads = []
        for j in jacobians:
            # Ignore the batch size and num output classes dimensions and
            # recover just the number of dimensions in the weights.
            num_weight_dims = len(j.shape) - 2

            # Make the scalars the same shape as the gradients so the
            # multiplication can be broadcasted.
            dJ_dz_exp = dJ_dz
            for _ in range(num_weight_dims):
                dJ_dz_exp = tf_shell.expand_dims(dJ_dz_exp, axis=-1)

            # Scale the jacobian.
            scaled_grad = dJ_dz_exp * j
            # ^ shape: [batch_size, num output classes, weights]

            # Sum over the output classes. At this point, this is a gradient
            # and no longer a jacobian.
            scaled_grad = tf_shell.reduce_sum(scaled_grad, axis=1)
            # ^ shape: [batch_size, weights]

            grads.append(scaled_grad)

        return grads

    def compute_postscale_precursors_ubatch(self, ubatch_features, scaling_factor):
        prediction, jacobians = self._predict_and_jacobian(ubatch_features)

        # Perform PostScale (dJ/dz * dz/dw) in plaintext for every
        # possible label using the worst casse quantization of the
        # jacobian.
        with tf.name_scope("sensitivity_analysis"):
            per_class_norms = tf.TensorArray(
                dtype=tf.keras.backend.floatx(),
                size=self.out_classes,
                dynamic_size=False,
                clear_after_read=False,
                element_shape=tf.TensorShape([ubatch_features.shape[0]]),
            )
            worst_case_jacobians = [
                tf_shell.worst_case_rounding(j, scaling_factor) for j in jacobians
            ]
            worst_case_prediction = tf_shell.worst_case_rounding(
                prediction, scaling_factor
            )

            def cond(possible_label_i, _):
                return possible_label_i < self.out_classes

            def body(possible_label_i, per_class_norms):
                possible_label = tf.one_hot(
                    possible_label_i,
                    self.out_classes,
                    dtype=tf.keras.backend.floatx(),
                )
                dJ_dz = worst_case_prediction - possible_label
                possible_grads = self._backward(dJ_dz, worst_case_jacobians)
                possible_norms = tf.vectorized_map(self.gradient_norms, possible_grads)

                per_class_norms = per_class_norms.write(
                    possible_label_i, possible_norms
                )
                return possible_label_i + 1, per_class_norms

            # Using a tf.while_loop (vs. a python for loop) is preferred
            # as it does not encode the unrolled loop into the graph and
            # also allows explicit control over the loop's parallelism.
            # Increasing parallel_iterations may be faster at the expense
            # of memory usage.
            possible_label_i = tf.constant(0)
            _, per_class_norms = tf.while_loop(
                cond,
                body,
                [possible_label_i, per_class_norms],
                parallel_iterations=1,
                shape_invariants=[
                    tf.TensorShape([]),
                    tf.TensorSpec(None, dtype=tf.variant),
                ],
            )

            per_class_per_example_norms = per_class_norms.stack()
            # ^ shape: [num output classes, batch_size]
            per_example_per_class_norms = tf.transpose(per_class_per_example_norms)
            # ^ shape: [batch_size, num output classes]
            return prediction, jacobians, per_example_per_class_norms

    def compute_postscale_precursors(self, features, scaling_factor, ubatch_size):
        # Split the features into ubatches and loop over them using a
        # tf.while_loop to avoid unrolling the loop into the graph,
        # preserving device memory.
        predictions_ta_dev = tf.TensorArray(
            dtype=tf.keras.backend.floatx(),
            size=self.ubatch_per_batch,
            dynamic_size=False,
            clear_after_read=False,
            element_shape=[ubatch_size, self.out_classes],
        )
        jacobians_tas_dev = [
            tf.TensorArray(
                dtype=v.dtype,
                size=self.ubatch_per_batch,
                dynamic_size=False,
                clear_after_read=False,
                element_shape=[ubatch_size, self.out_classes] + v.shape,
            )
            for v in self.trainable_variables
        ]
        jacobians_norms_ta_dev = tf.TensorArray(
            dtype=tf.keras.backend.floatx(),
            size=self.ubatch_per_batch,
            dynamic_size=False,
            clear_after_read=False,
            element_shape=[ubatch_size, self.out_classes],
        )

        def cond(i, *args):
            return i < self.ubatch_per_batch

        def body(i, predictions_ta, jacobians_tas, jacobians_norms_ta):
            prediction, jacobians, jacobians_norms = (
                self.compute_postscale_precursors_ubatch(
                    features[i],
                    scaling_factor,
                )
            )

            # Store the results in the TensorArrays.
            predictions_ta = predictions_ta.write(i, prediction)
            for j, jac in enumerate(jacobians):
                jacobians_tas[j] = jacobians_tas[j].write(i, jac)
            jacobians_norms_ta = jacobians_norms_ta.write(i, jacobians_norms)
            return (
                i + 1,
                predictions_ta,
                jacobians_tas,
                jacobians_norms_ta,
            )

        i = tf.constant(0)
        (
            _,
            predictions_ta_dev,
            jacobians_tas_dev,
            jacobians_norms_ta_dev,
        ) = tf.while_loop(
            cond,
            body,
            [
                i,
                predictions_ta_dev,
                jacobians_tas_dev,
                jacobians_norms_ta_dev,
            ],
            parallel_iterations=1,
        )

        # Concatenate the TensorArrays over the ubatch dimension.
        return (
            predictions_ta_dev.concat(),
            [layer.concat() for layer in jacobians_tas_dev],
            jacobians_norms_ta_dev.concat(),
        )

    def compute_grads(self, features, enc_labels):
        scaling_factor = (
            enc_labels.scaling_factor
            if hasattr(enc_labels, "scaling_factor")
            else float("inf")
        )
        scaling_factor = tf.cast(scaling_factor, dtype=tf.keras.backend.floatx())

        with tf.device(self.features_party_dev):
            num_devices = len(self.jacobian_devices)
            batch_size = features.shape[0]
            if batch_size % num_devices != 0:
                raise ValueError("Batch size must be divisible by number of devices.")
            batch_size_per_device = batch_size // num_devices
            if batch_size_per_device % self.ubatch_per_batch != 0:
                raise ValueError(
                    "Batch size (per device) must be divisible by ubatch_per_batch."
                )
            ubatch_size = batch_size_per_device // self.ubatch_per_batch
            num_ubatches = num_devices * self.ubatch_per_batch

            split_features = tf.reshape(
                features,
                [num_devices, self.ubatch_per_batch, ubatch_size] + features.shape[1:],
            )

        ### Computing the jacobians can be done in two ways. First, the simplest
        ### approach is to iterate over the available devices and concat the
        ### results. This is what's below, commented out. For some models,
        ### Tensorflow wants to execute the code serially (first set of features
        ### on GPU0 then next set on GPU1, etc).
        ### Instead, the tf.distribute.strategy can be used to distribute the
        ### computation across the available devices and avoids this problem
        ### at the cost of the caller needing to create the model with the
        ### strategy scope (increase complexity for calling code).
        ###
        # # Create TensorArrays for collecting results.
        # predictions_dev = []
        # jacobians_dev = []
        # jacobians_norms_dev = []

        # # Split the batch of features across the available devices and compute
        # # the PostScale precursors.
        # for d, device in enumerate(self.jacobian_devices):
        #     with tf.device(device):
        #         # Compute the PostScale precursors for the device's features.
        #         (
        #             predictions_one_dev,
        #             jacobians_one_dev,
        #             jacobians_norms_one_dev,
        #         ) = self.compute_postscale_precursors(
        #             tf.identity(split_features[d]),
        #             tf.identity(scaling_factor),
        #             ubatch_size
        #         )

        #         predictions_dev.append(predictions_one_dev)
        #         jacobians_dev.append(jacobians_one_dev)
        #         jacobians_norms_dev.append(jacobians_norms_one_dev)

        # with tf.device(self.features_party_dev):
        #     # Copy from GPUs back to features_party
        #     predictions_dev = [tf.identity(p) for p in predictions_dev]
        #     jacobians_dev = [
        #         [tf.identity(layer) for layer in jacobians]
        #         for jacobians in jacobians_dev
        #     ]
        #     jacobians_norms_dev = [tf.identity(n) for n in jacobians_norms_dev]

        #     # Concatenate the Tensors for each device.
        #     predictions = tf.concat(predictions_dev, axis=0)

        #     # jacobians_dev shape is:
        #     # [num_devices][layers][Tensor of ubatches]
        #     full_jacobians = []
        #     for layer_jacobians in zip(*jacobians_dev):
        #         full_jacobians.append(tf.concat(layer_jacobians, axis=0))
        #     # full_jacobians shape is:
        #     # [layers][Tensor of batches]

        #     jacobians_norms_dev = tf.concat(jacobians_norms_dev, axis=0)

        # Build per-replica input from the pre-split tensor
        per_replica_features = (
            self.jacobian_strategy.experimental_distribute_values_from_function(
                lambda ctx: split_features[ctx.replica_id_in_sync_group]
            )
        )

        # One per-replica call; returns PerReplica structures
        per_replica_out = self.jacobian_strategy.run(
            lambda feats, sf, ub: self.compute_postscale_precursors(feats, sf, ub),
            args=(per_replica_features, scaling_factor, ubatch_size),
        )

        # Unpack and gather (concatenate across replicas on axis 0)
        preds_pr, jacs_pr_list, norms_pr = per_replica_out

        predictions = self.jacobian_strategy.gather(preds_pr, axis=0)  # Tensor

        # jacs_pr_list is a Python list of PerReplica values
        jacobians = [
            self.jacobian_strategy.gather(j_pr, axis=0) for j_pr in jacs_pr_list
        ]  # List[Tensor]

        jacobians_norms = self.jacobian_strategy.gather(norms_pr, axis=0)  # Tensor

        with tf.device(self.features_party_dev):
            per_example_norms = tf.reduce_max(jacobians_norms, axis=1)

            if type(enc_labels) is tf.Tensor:
                tf.debugging.assert_equal(
                    tf.shape(predictions),
                    tf.shape(enc_labels),
                    message="Predictions and labels must have the same shape.",
                )
            # The base class ensures that when the loss is CCE, the last
            # layer's activation is softmax. The derivative of these two
            # functions is simple subtraction.
            dJ_dz = enc_labels.__rsub__(predictions)
            # ^ shape: [batch_size, num output classes]

            grads = self._backward(dJ_dz, jacobians)

            # Manually set shapes because the distributed.strategy breaks
            # shape inference.
            # Recall jacobians shape is:
            # layers list x tensor of shape (batch size, num output classes, weights)
            for g, w in zip(grads, self.trainable_variables):
                if isinstance(g, tf_shell.ShellTensor64):
                    g._raw_tensor.set_shape(w.shape)  # batch dim is implicit
                else:
                    g.set_shape([batch_size] + w.shape)

            per_example_norms.set_shape([batch_size])
            predictions.set_shape([batch_size, self.out_classes])

        return grads, per_example_norms, predictions
